import os
import re
import json
import sys

import arxiv
import yaml
import logging
import argparse
import datetime
import requests

import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf8')
logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)

base_url = "https://arxiv.paperswithcode.com/api/v0/papers/"
github_url = "https://api.github.com/search/repositories"
arxiv_url = "http://arxiv.org/"


def load_config(config_file: str) -> dict:
    '''
    config_file: input config file path
    return: a dict of configuration
    '''

    # make filters pretty
    def pretty_filters(**config) -> dict:
        keywords = dict()
        EXCAPE = '\"'
        QUOTA = ''  # NO-USE
        OR = ' OR '  # TODO

        def parse_filters(filters: list):
            ret = ''
            for idx in range(0, len(filters)):
                filter = filters[idx]
                if len(filter.split()) > 1:
                    ret += (EXCAPE + filter + EXCAPE)
                else:
                    ret += (QUOTA + filter + QUOTA)
                if idx != len(filters) - 1:
                    ret += OR
            return ret

        for k, v in config['keywords'].items():
            keywords[k] = parse_filters(v['filters'])
        return keywords

    with open(config_file, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
        config['kv'] = pretty_filters(**config)
        logging.info(f'config = {config}')
    return config


def get_authors(authors, first_author=False):
    output = str()
    if first_author == False:
        output = ", ".join(str(author) for author in authors)
    else:
        output = authors[0]
    return output


def sort_papers(papers):
    output = dict()
    keys = list(papers.keys())
    keys.sort(reverse=True)
    for key in keys:
        output[key] = papers[key]
    return output


import requests


def get_code_link(qword: str) -> str:
    """
    This short function was auto-generated by ChatGPT.
    I only renamed some params and added some comments.
    @param qword: query string, eg. arxiv ids and paper titles
    @return paper_code in github: string, if not found, return None
    """
    # query = f"arxiv:{arxiv_id}"
    query = f"{qword}"
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc"
    }
    r = requests.get(github_url, params=params)
    results = r.json()
    code_link = None
    if results["total_count"] > 0:
        code_link = results["items"][0]["html_url"]
    return code_link


def get_daily_papers(topic, query="slam", max_results=200):
    """
    @param topic: str
    @param query: str
    @return paper_with_code: dict
    """
    # output
    content = dict()
    content_to_web = dict()
    search_engine = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.Relevance
    )

    for result in search_engine.results():

        paper_id = result.get_short_id()
        paper_title = result.title
        paper_url = result.entry_id
        code_url = base_url + paper_id  # TODO
        paper_abstract = result.summary.replace("\n", " ")
        paper_authors = get_authors(result.authors)
        paper_first_author = get_authors(result.authors, first_author=True)
        primary_category = result.primary_category
        publish_time = result.published.date()
        update_time = result.updated.date()
        comments = result.comment

        logging.info(f"Time = {update_time} title = {paper_title} author = {paper_first_author}")

        # eg: 2108.09112v1 -> 2108.09112
        ver_pos = paper_id.find('v')
        if ver_pos == -1:
            paper_key = paper_id
        else:
            paper_key = paper_id[0:ver_pos]
        paper_url = arxiv_url + 'abs/' + paper_key

        try:
            # source code link
            r = requests.get(code_url).json()
            repo_url = None
            if "official" in r and r["official"]:
                repo_url = r["official"]["url"]
            # TODO: not found, two more chances
            # else:
            #    repo_url = get_code_link(paper_title)
            #    if repo_url is None:
            #        repo_url = get_code_link(paper_key)
            if repo_url is not None:
                content[paper_key] = "|**{}**|**{}**|{} et.al.|[{}]({})|**[link]({})**|\n".format(
                    update_time, paper_title, paper_first_author, paper_key, paper_url, repo_url)
                content_to_web[paper_key] = "- {}, **{}**, {} et.al., Paper: [{}]({}), Code: **[{}]({})**".format(
                    update_time, paper_title, paper_first_author, paper_url, paper_url, repo_url, repo_url)

            else:
                content[paper_key] = "|**{}**|**{}**|{} et.al.|[{}]({})|null|\n".format(
                    update_time, paper_title, paper_first_author, paper_key, paper_url)
                content_to_web[paper_key] = "- {}, **{}**, {} et.al., Paper: [{}]({})".format(
                    update_time, paper_title, paper_first_author, paper_url, paper_url)

            # TODO: select useful comments
            comments = None
            if comments != None:
                content_to_web[paper_key] += f", {comments}\n"
            else:
                content_to_web[paper_key] += f"\n"

        except Exception as e:
            logging.error(f"exception: {e} with id: {paper_key}")

    data = {topic: content}
    data_web = {topic: content_to_web}
    return data, data_web


def update_paper_links(filename):
    '''
    weekly update paper links in json file
    '''

    def parse_arxiv_string(s):
        parts = s.split("|")
        date = parts[1].strip()
        title = parts[2].strip()
        authors = parts[3].strip()
        arxiv_id = parts[4].strip()
        code = parts[5].strip()
        arxiv_id = re.sub(r'v\d+', '', arxiv_id)
        return date, title, authors, arxiv_id, code

    with open(filename, "r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)

        json_data = m.copy()

        for keywords, v in json_data.items():
            logging.info(f'keywords = {keywords}')
            for paper_id, contents in v.items():
                contents = str(contents)

                update_time, paper_title, paper_first_author, paper_url, code_url = parse_arxiv_string(contents)

                contents = "|{}|{}|{}|{}|{}|\n".format(update_time, paper_title, paper_first_author, paper_url,
                                                       code_url)
                json_data[keywords][paper_id] = str(contents)
                logging.info(f'paper_id = {paper_id}, contents = {contents}')

                valid_link = False if '|null|' in contents else True
                if valid_link:
                    continue
                try:
                    code_url = base_url + paper_id  # TODO
                    r = requests.get(code_url).json()
                    repo_url = None
                    if "official" in r and r["official"]:
                        repo_url = r["official"]["url"]
                        if repo_url is not None:
                            new_cont = contents.replace('|null|', f'|**[link]({repo_url})**|')
                            logging.info(f'ID = {paper_id}, contents = {new_cont}')
                            json_data[keywords][paper_id] = str(new_cont)

                except Exception as e:
                    logging.error(f"exception: {e} with id: {paper_id}")
        # dump to json file
        with open(filename, "w") as f:
            json.dump(json_data, f)


def update_json_file(filename, data_dict):
    '''
    daily update json file using data_dict
    '''

    with open(filename, "r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)

    json_data = m.copy()

    # update papers in each keywords
    for data in data_dict:
        for keyword in data.keys():
            papers = data[keyword]

            if keyword in json_data.keys():
                json_data[keyword].update(papers)
            else:
                json_data[keyword] = papers

    with open(filename, "w") as f:
        json.dump(json_data, f)


def json_to_html(filename, md_filename,
               task='',
               to_web=False,
               use_title=True,
               use_tc=True,
               show_badge=True,
               use_b2t=True):
    """
    @param filename: str
    @param md_filename: str - 现在是HTML文件名
    @return None
    """

    def pretty_math(s: str) -> str:
        ret = ''
        match = re.search(r"\$.*\$", s)
        if match == None:
            return s
        math_start, math_end = match.span()
        space_trail = space_leading = ''
        if s[:math_start][-1] != ' ' and '*' != s[:math_start][-1]: space_trail = ' '
        if s[math_end:][0] != ' ' and '*' != s[math_end:][0]: space_leading = ' '
        ret += s[:math_start]
        ret += f'{space_trail}${match.group()[1:-1].strip()}${space_leading}'
        ret += s[math_end:]
        return ret

    DateNow = datetime.date.today()
    DateNow = str(DateNow)
    DateNow = DateNow.replace('-', '.')

    with open(filename, "r") as f:
        content = f.read()
        if not content:
            data = {}
        else:
            data = json.loads(content)

    # 创建新的HTML文件
    with open(md_filename, "w+", encoding='utf-8') as f:
        # 写入HTML头部
        f.write("<!DOCTYPE html>\n")
        f.write("<html>\n<head>\n")
        f.write("<meta charset='utf-8'>\n")
        f.write("<meta name='viewport' content='width=device-width, initial-scale=1.0'>\n")
        f.write("<title>ArXiv Papers Daily</title>\n")
        f.write("<style>\n")
        f.write("""
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
                line-height: 1.6;
                margin: 0;
                padding: 20px;
                background-color: #ffffff;
                color: #333;
            }
            .container {
                max-width: 1100px;
                margin: 0 auto;
            }
            h1 {
                color: #2c3e50;
                text-align: center;
                padding: 20px 0;
                margin-bottom: 20px;
                border-bottom: 1px solid #eee;
            }
            h2 {
                color: #2c3e50;
                padding: 10px 0;
                margin-top: 30px;
                border-bottom: 1px solid #eee;
            }
            table {
                border-collapse: collapse;
                width: 100%;
                margin: 15px 0;
            }
            th, td {
                padding: 10px;
                text-align: left;
                border-bottom: 1px solid #eee;
            }
            th {
                background-color: #f8f9fa;
                font-weight: 500;
            }
            tr:hover {
                background-color: #f8f9fa;
            }
            a {
                color: #2980b9;
                text-decoration: none;
            }
            a:hover {
                text-decoration: underline;
            }
            .back-to-top {
                position: fixed;
                bottom: 20px;
                right: 20px;
                background-color: #f8f9fa;
                color: #666;
                padding: 8px 12px;
                border-radius: 4px;
                text-decoration: none;
                font-size: 14px;
                border: 1px solid #eee;
            }
            details {
                margin-bottom: 20px;
            }
            summary {
                cursor: pointer;
                color: #2c3e50;
                margin-bottom: 10px;
            }
            details ul {
                margin: 10px 0;
                padding-left: 20px;
            }
            details li {
                margin: 5px 0;
            }
            .pdf-link, .code-link {
                padding: 2px 6px;
                border-radius: 3px;
                font-size: 0.9em;
            }
            .pdf-link {
                color: #e74c3c;
            }
            .code-link {
                color: #27ae60;
            }
            @media (max-width: 768px) {
                body {
                    padding: 10px;
                }
                th, td {
                    padding: 8px;
                }
            }
            /* 优化链接样式 */
            .title-link {
                color: #2c3e50;
                font-weight: 500;
                transition: color 0.2s;
            }
            .title-link:hover {
                color: #3498db;
            }
            .link-btn {
                display: inline-block;
                padding: 4px 8px;
                border-radius: 4px;
                text-decoration: none;
                font-size: 0.9em;
                transition: all 0.2s;
            }
            .pdf-link {
                background-color: #f8f9fa;
                color: #e74c3c;
                border: 1px solid #e74c3c;
            }
            .pdf-link:hover {
                background-color: #e74c3c;
                color: white;
                text-decoration: none;
            }
            .code-link {
                background-color: #f8f9fa;
                color: #27ae60;
                border: 1px solid #27ae60;
            }
            .code-link:hover {
                background-color: #27ae60;
                color: white;
                text-decoration: none;
            }
            /* 当链接不可用时的样式 */
            .link-btn.disabled {
                opacity: 0.5;
                cursor: not-allowed;
                border-color: #ccc;
                color: #999;
            }
            /* 添加直达按钮样式 */
            .goto-btn {
                display: inline-block;
                padding: 2px 6px;
                margin-left: 8px;
                background-color: #f8f9fa;
                border: 1px solid #dee2e6;
                border-radius: 3px;
                color: #495057;
                text-decoration: none;
                font-size: 12px;
            }
            .goto-btn:hover {
                background-color: #e9ecef;
                text-decoration: none;
            }
        """)
        f.write("</style>\n")
        f.write("</head>\n<body>\n")
        f.write("<div class='container'>\n")

        if use_title:
            f.write(f"<h1>Updated on {DateNow}</h1>\n")

        # 目录
        if use_tc:
            f.write("<details>\n")
            f.write("<summary>Table of Contents</summary>\n")
            f.write("<ul>\n")
            for keyword in data.keys():
                if data[keyword]:
                    kw = keyword.replace(' ', '-')
                    f.write(f"<li><a href='#{kw.lower()}'>{keyword}</a></li>\n")
            f.write("</ul>\n")
            f.write("</details>\n")

        # 内容部分
        for keyword in data.keys():
            day_content = data[keyword]
            if not day_content:
                continue

            f.write(f"<h2 id='{keyword.lower()}'>{keyword}</h2>\n")

            if use_title:
                f.write("<table>\n")
                f.write("<tr><th>Publish Date</th><th>Title</th><th>Authors</th><th>PDF</th><th>Code</th></tr>\n")

            # 按日期排序论文
            day_content = sort_papers(day_content)

            for _, v in day_content.items():
                if v is not None:
                    # 解析原始数据
                    parts = v.split('|')
                    if len(parts) >= 5:
                        date = parts[1].strip().replace('**', '')
                        title_part = parts[2].strip().replace('**', '')
                        authors = parts[3].strip()

                        # 处理PDF链接
                        pdf_link_match = re.search(r'\[(\d+\.\d+)\]', parts[4])
                        if pdf_link_match:
                            arxiv_id = pdf_link_match.group(1)
                            pdf_link = f"http://arxiv.org/pdf/{arxiv_id}"
                        else:
                            pdf_link = parts[4].strip()

                        # 处理代码链接
                        code_link_match = re.search(r'\[link\]\((.*?)\)', parts[5]) if len(parts) > 5 else None
                        if code_link_match:
                            code_link = code_link_match.group(1)
                        else:
                            code_link = parts[5].strip() if len(parts) > 5 else ''

                        code_link = code_link.replace('**', '')

                        title_match = re.search(r'\[([^\]]+)\]\(([^\)]+)\)', title_part)
                        if title_match:
                            title = title_match.group(1).replace('**', '')
                            arxiv_link = title_match.group(2)
                            if not arxiv_link.startswith('http'):
                                arxiv_link = f"https://arxiv.org{arxiv_link}"
                        else:
                            title = title_part
                            arxiv_link = ''

                        f.write("<tr>")
                        f.write(f"<td>{date}</td>")
                        f.write(f"<td>{title}<br><small>{arxiv_link}</small></td>")
                        f.write(f"<td>{authors}</td>")

                        # PDF链接添加直达按钮
                        f.write(f"<td>{pdf_link} <a href='{pdf_link}' target='_blank' class='goto-btn'>前往</a></td>")

                        # Code链接，只在有效链接时添加前往按钮
                        if code_link:
                            f.write(
                                f"<td>{code_link} <a href='{code_link}' target='_blank' class='goto-btn'>前往</a></td>")
                        else:
                            f.write("<td>-</td>")

                        f.write("</tr>\n")

            if use_title:
                f.write("</table>\n")

            if use_b2t:
                f.write("<a href='#top' class='back-to-top'>↑ Top</a>\n")

        # 结束容器和body
        f.write("</div>\n")
        f.write("</body>\n</html>")

    logging.info(f"{task} finished")


def demo(**config):
    # TODO: use config
    data_collector = []
    data_collector_web = []

    keywords = config['kv']
    max_results = config['max_results']
    publish_html = config['publish_html']
    show_badge = config['show_badge']

    b_update = config['update_paper_links']
    logging.info(f'Update Paper Link = {b_update}')
    if config['update_paper_links'] == False:
        logging.info(f"GET daily papers begin")
        for topic, keyword in keywords.items():
            logging.info(f"Keyword: {topic}")
            data, data_web = get_daily_papers(topic, query=keyword,
                                              max_results=max_results)
            data_collector.append(data)
            data_collector_web.append(data_web)
            print("\n")
        logging.info(f"GET daily papers end")

    # 1. update README.md file
    if publish_html:
        json_file = config['json_readme_path']
        html_file = config['path']
        # update paper links
        if config['update_paper_links']:
            update_paper_links(json_file)
        else:
            # update json data
            update_json_file(json_file, data_collector)
        # json data to markdown
        json_to_html(json_file, html_file, task='Update Readme', \
                   show_badge=show_badge)




if __name__ == "__main__":
    config_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.yaml')
    config = load_config(config_file)

    config = {**config, 'update_paper_links': False}
    demo(**config)
